{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1lBxUeUae534DZi6PgrIrWuttqhqNX-XA",
      "authorship_tag": "ABX9TyMGenUg23dvv41i/1PE7+JC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/np03cs5a250002-cmyk/concept-and-technology-of-ai/blob/main/worksheet4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UzL8R0WR2fZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the Dataset:\n",
        "df= pd.read_csv(\"/content/drive/MyDrive/diabetes_.csv\")\n",
        "#df.head()\n",
        "#df.tail()\n",
        "#df.describe()\n",
        "null_values= df.isnull().sum()\n",
        "print(f\"{null_values}\")"
      ],
      "metadata": {
        "id": "8gleZCM_TzwQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97361df4-be58-4b78-ed7b-ccaa655a9482"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pregnancies                 0\n",
            "Glucose                     0\n",
            "BloodPressure               0\n",
            "SkinThickness               0\n",
            "Insulin                     0\n",
            "BMI                         0\n",
            "DiabetesPedigreeFunction    0\n",
            "Age                         0\n",
            "Outcome                     0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle any missing values appropriately, either by dropping or imputing them based on the data.\n",
        "missing_values= df.isnull().sum()/len(df)*100\n",
        "for column in df.columns:\n",
        "  if missing_values[column]>10:\n",
        "    df[column].fillna(df[column].mean(),inplace=True)\n",
        "  else:\n",
        "    df.dropna(subset=[column],inplace=True)\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "FVwItm74T3_l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "251773ba-aeee-4f7a-8fa9-085189621537"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pregnancies                 0\n",
            "Glucose                     0\n",
            "BloodPressure               0\n",
            "SkinThickness               0\n",
            "Insulin                     0\n",
            "BMI                         0\n",
            "DiabetesPedigreeFunction    0\n",
            "Age                         0\n",
            "Outcome                     0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X= df.drop(columns=\"Outcome\").values\n",
        "Y= df[\"Outcome\"].values\n",
        "#print(X.shape[0])\n",
        "#print(int(len(X) *0.3))\n",
        "def train_test_split_scratch(X,Y, test_size=0.3, seed=42):\n",
        "   #the sequence of random numbers will always be the same every time you run the program\n",
        "\n",
        "   np.random.seed(seed)\n",
        "   indices= np.arange(X.shape[0]) #prints the number from 0 to 767\n",
        "   np.random.shuffle(indices) #shuffle\n",
        "   test_split_size= int(len(X) *test_size)\n",
        "   test_idx= indices[:test_split_size]\n",
        "   train_idx= indices[test_split_size:]\n",
        "   X_train= X[train_idx]\n",
        "   X_test= X[test_idx]\n",
        "   Y_train=Y[train_idx]\n",
        "   Y_test=Y[test_idx]\n",
        "   return X_train,X_test,Y_train,Y_test\n",
        "X_train, X_test, Y_train, Y_test = train_test_split_scratch(X, Y)\n",
        "print(f\"Xtrain shape {X_train.shape}\")\n",
        "print(f\"Xtest shape {X_test.shape}\")\n",
        "print(f\"Ytrain shape {Y_train.shape}\")\n",
        "print(f\"Ytest shape {Y_test.shape}\")\n",
        "\n",
        "\n",
        "# implementing KNN\n",
        "def Encludiean_distance(a,b):\n",
        "  return np.sqrt(np.sum((a-b)**2))\n",
        "\n",
        "#Predicting the class for a single query.\n",
        "def knn_predict_single (x, X_train, Y_train,k=3):\n",
        "  distances = []\n",
        "  for point in X_train:\n",
        "      distances.append(Encludiean_distance(x, point))\n",
        "  sorted_indx= np.argsort(distances)\n",
        "  nearest_index=sorted_indx[:k]\n",
        "  nearest_labels= Y_train[nearest_index]\n",
        "  #finds the most frequent value in an array\n",
        "\n",
        "  prediction= np.bincount(nearest_labels).argmax()\n",
        "  return prediction\n",
        "\n",
        "#Predicting classes for all test samples.\n",
        "def knn_predict(X_test, X_train , Y_train, k=3):\n",
        "  preds=[]\n",
        "  for sample in X_test:\n",
        "      preds.append(knn_predict_single(sample, X_train, Y_train,k ))\n",
        "  return np.array(preds)\n",
        "\n",
        "#performance accuracy\n",
        "def compute_accuracy(y_true, y_pred):\n",
        "  correctvalues= np.sum(y_true==y_pred)\n",
        "  return (correctvalues/len(y_true)*100)\n",
        "\n",
        "predictions = knn_predict(X_test, X_train, Y_train, k=5)\n",
        "\n",
        "accuracy = compute_accuracy(Y_test, predictions)\n",
        "print(\" Predictions:\", predictions[:5])\n",
        "print(f\"the accuracy is {accuracy} %\")"
      ],
      "metadata": {
        "id": "uGlmRTi9T_uF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "problem 2\n",
        "Xscaled​= X-mean(x) /std(x)"
      ],
      "metadata": {
        "id": "DHxDi9miUEQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_mean= X_train.mean(axis=0)\n",
        "X_std= X_train.std(axis=0)\n",
        "X_std[X_std==0]=1\n",
        "X_train_scaled=(X_train -  X_mean)/X_std\n",
        "X_test_scaled=(X_test -  X_mean)/X_std\n",
        "#print(f\"x train scaled is \\n {X_train_scaled[:3]}\")\n",
        "#print(f\"x test scaled is \\n {X_test_scaled[:3]}\")\n",
        "prediction_scale= knn_predict(X_test_scaled,X_train_scaled,Y_train, k=3)\n",
        "accuracy_scale= compute_accuracy(Y_test,prediction_scale )\n",
        "print(f\"before unscaled prediction {accuracy}\")\n",
        "print(f\"after scaled prediction {accuracy_scale}\")\n",
        "\n",
        "#• Discuss:\n",
        "# How scaling impacted the KNN performance.\n",
        "# The reason for any observed changes in accuracy.\n",
        "print(f''' \\n here before scaling the accuracy is {accuracy}\n",
        "but after scaling the prediction is {accuracy_scale}\n",
        "so the accuracy differ by {accuracy_scale-accuracy}''')\n",
        "print('''\n",
        "the accuracy increased becuase:\n",
        "- More meaningful nearest neighbors\n",
        "- Better separation between classes\n",
        "- Improved accuracy and generalization\n",
        "\n",
        "\n",
        "''')"
      ],
      "metadata": {
        "id": "3gjXhyt_UFSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem - 3 - Experimentation with k:"
      ],
      "metadata": {
        "id": "Jc7pjruIUNya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k_values= range(1,16)\n",
        "print(k_values)\n",
        "import time\n",
        "scaled_accuracy= []\n",
        "unscaled_accuracy= []\n",
        "time_unscaled=[]\n",
        "time_scaled=[]\n",
        "for kk in k_values:\n",
        "  #unscalad\n",
        "  unscaled_pred= knn_predict(X_test,X_train,Y_train, kk)\n",
        "  unscaled_accuracy.append(compute_accuracy(Y_test, unscaled_pred))\n",
        "\n",
        "\n",
        "  #scaled\n",
        "  scaled_pred= knn_predict(X_test_scaled,X_train_scaled,Y_train, kk)\n",
        "  scaled_accuracy.append(compute_accuracy(Y_test, scaled_pred))\n",
        "\n",
        "  #time\n",
        "\n",
        "  #unscaled\n",
        "  t1= time.time()\n",
        "  knn_predict(X_test,X_train,Y_train, kk)\n",
        "  t2= time.time()\n",
        "  time_unscaled.append(t2 - t1)\n",
        "  #scaled\n",
        "  t3= time.time()\n",
        "  knn_predict(X_test_scaled,X_train_scaled,Y_train, kk)\n",
        "  t4= time.time()\n",
        "  time_scaled.append(t4-t3)\n",
        "\n",
        "# plotting graph\n",
        "plt.plot(k_values,unscaled_accuracy, marker='o',label='unscaled', color='red')\n",
        "plt.plot(k_values,scaled_accuracy, marker='o',label='scaled')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"K vs Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D6eagyqoUKWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(k_values, time_unscaled, marker='o', label='Unscaled_time')\n",
        "plt.plot(k_values, time_scaled, marker='x', label='Scaled_time')\n",
        "plt.xlabel(\"k\")\n",
        "plt.ylabel(\"Time (seconds)\")\n",
        "plt.title(\"k vs Time\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IjgajddkV1Ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTkAjGMr2bMz"
      },
      "source": [
        "• Discuss how the choice of k affects the accuracy and computational cost.\n",
        "\n",
        "As we see, when the k is too small (1,2,3..) accuracy is usually lower and unstable becuase the model become too sensitive and it reads the noise too so one wrong neighbour can change the prediction. so, when the k is increase the accuracy gradually improves and become more stable becuase more neighbours vote reduce the influence of noise.\n",
        "\n",
        "In conclusion:\n",
        "when the value of k increase, the accuracy is also increasing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Identify the optimal k based on your analysis.\n",
        "\n",
        "the highest accuracy occurs around k= 15 because the scaled accuracy is more than 75%-80% so we can say that optimal k is 12-15"
      ],
      "metadata": {
        "id": "JhnN99OHWd0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem - 4 - Additional Questions {Optional - But Highly Recommended}:\n",
        "\n",
        "• Discuss the challenges of using KNN for large datasets and high-dimensional data.\n",
        "\n",
        "challenges are:\n",
        "\n",
        "kNN can be computationally expensive due to the need to calculate distances for all training examples. Storage and memory requirements can be significant, especially with pre-sorting and large datasets. High-dimensional data exacerbates computational costs and performance due to the ”curse of dimensionality.” Efficient data handling techniques (e.g., LSH, condensing) and dimensionality reduction methods are essential for optimizing kNN performance. When d ≫ 0 (i.e., when the number of dimensions is very large), points drawn from a probability distribution start to lose similarity, causing the kNN assumption to break down."
      ],
      "metadata": {
        "id": "KwHE9zXwWqMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "• Suggest strategies to improve the efficiency of KNN (e.g., approximate nearest neighbors, dimensionality\n",
        "reduction)\n",
        "\n",
        "improvement are:\n",
        "\n",
        "Dimensionality reduction\n",
        "\n",
        "use efficient data structure\n",
        "\n",
        "reduce dataset\n",
        "\n",
        "remove irrelevant or noisy features\n",
        "\n"
      ],
      "metadata": {
        "id": "AHoZK4raWsko"
      }
    }
  ]
}
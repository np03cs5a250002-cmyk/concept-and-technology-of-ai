{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMZYGnIwgYBoXWZgH+Jh3Rp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/np03cs5a250002-cmyk/concept-and-technology-of-ai/blob/main/worksheet2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-xjApzGkmwX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1 - Data Read, Write and Inspect: • Dataset for the Task: \"bank.csv\""
      ],
      "metadata": {
        "id": "szvdgPfIpCl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Load the provided dataset and import in pandas DataFrame.\n",
        "df=pd.read_csv(\"/content/drive/MyDrive/bank.csv\")\n",
        "\n",
        "#2. Check info of the DataFrame and identify following:\n",
        "# df.head()\n",
        "# df.tail()\n",
        "# df.info()\n",
        "\n",
        "#a\n",
        "print(\"Columns with dtype = object:\")\n",
        "object_cols = df.select_dtypes(include='object').columns\n",
        "print(object_cols)\n",
        "#b\n",
        "for col in object_cols:\n",
        "    print(f\" \\n unique in column {col}: \")\n",
        "    print(df[col].unique())\n",
        "#c\n",
        "total_val= df.isnull().sum();\n",
        "print(total_val)\n",
        "\n",
        "#3\n",
        "drop_column= df.drop(columns=object_cols)\n",
        "drop_column.to_csv(\"banknumericdata.csv\",index=True)\n",
        "print(\"sucessfully written\")\n",
        "\n",
        "\n",
        "#4\n",
        "new_csv= pd.read_csv(\"/content/banknumericdata.csv\")\n",
        "print(new_csv.describe())"
      ],
      "metadata": {
        "id": "Y8SngPI9lgAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sGCHb4YOpTJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2 - Data Imputations:"
      ],
      "metadata": {
        "id": "uJiHTa_XpY5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem 2:\n",
        "df1= pd.read_csv(\"/content/drive/MyDrive/medical_students_dataset.csv\")\n",
        "# df1.head()\n",
        "# df1.tail()\n",
        "# df1.info()\n",
        "#1. Load the provided dataset and import in pandas DataFrame.\n",
        "dataset=pd.DataFrame(df1)\n",
        "# dataset.head()\n",
        "\n",
        "#2. Check info of the DataFrame and identify column with missing (null) values.\n",
        "dataset.info()\n",
        "total_null= dataset.isnull().sum()\n",
        "print(f\"total null is \\n {total_null}\")\n",
        "\n",
        "#3. For the column with missing values fill the values using various techniques we discussed above. Tryto explain why did you select the particular methods for particular column.\n",
        "dataset.ffill(inplace=True)\n",
        "#i used ffill becuase it is easier to use and it take the value of previous row\n",
        "\n",
        "#4 4. Check for any duplicate values present in Dataset and do necessary to manage the duplicate items.{Hint: dataset.duplicated.sum()}\n",
        "total_duplicate= dataset.duplicated().sum()\n",
        "print(f\"total duplicates values is {total_duplicate}\")\n",
        "#to remove duplicates we have to drop it\n",
        "#inplace= true make change in original dataset\n",
        "dataset.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "zlNpV1PlpWI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2 Exercises - Data Cleaning and Transformations with ”Titanic Dataset”:"
      ],
      "metadata": {
        "id": "qjGZtbj_phaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#problem 1:\n",
        "\n",
        "ds= pd.read_csv(\"/content/drive/MyDrive/Titanic-Dataset.csv\")\n",
        "ds= pd.DataFrame(ds)\n",
        "selected_columns = ds[[\"Name\", \"Pclass\",\"Sex\", \"Age\", \"Fare\",\"Survived\"]]\n",
        "\n",
        "# selected_columns.head()\n",
        "p1= selected_columns[selected_columns[\"Pclass\"]==1]\n",
        "#it displays the columns having Pclass =1\n",
        "pd.DataFrame(p1)\n",
        "#prints the mean median and others of Fare columns only\n",
        "p1.describe().Fare\n",
        "\n",
        " #or we can also do manually\n",
        "mean_fare = p1[\"Fare\"].mean()\n",
        "median_fare = p1[\"Fare\"].median()\n",
        "mode_fare = p1[\"Fare\"].mode()\n",
        "\n",
        "\n",
        "print(\"Mean Fare \", mean_fare)\n",
        "print(\"Median Fare:\", median_fare)\n",
        "print(\"Modee Fare:\", mode_fare)\n",
        "\n",
        "#Problem - 2:\n",
        "Null_ages= selected_columns[\"Age\"].isnull().sum()\n",
        "print(f\"there are {Null_ages} null values in age column\")\n",
        "# # # #dropping\n",
        "#drops the row of age if it has null values\n",
        "p1.dropna(subset=[\"Age\"])"
      ],
      "metadata": {
        "id": "oxrIIJUYpmKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Problem - 3:\n",
        "Embarked_cols= pd.get_dummies(ds[\"Embarked\"], prefix=\"Embarked\")\n",
        "#Add these new columns to the original DataFrame.\n",
        "ds= pd.concat([ds, Embarked_cols], axis=1)\n",
        "ds.head(2)\n",
        "\n",
        "#3. Drop the original ’Embarked’ column.\n",
        "ds.drop(columns=\"Embarked\")\n",
        "#4. Print the first few rows of the modified DataFrame to verify the changes.\n",
        "# ds.head(5)\n",
        "# it is sucessfully modified"
      ],
      "metadata": {
        "id": "myb-Ezh7pqWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Problem - 4:\n",
        "\n",
        "mean_survival_by_sex = ds.groupby('Sex')['Survived'].mean()\n",
        "print(mean_survival_by_sex)\n",
        "# visual representation is\n",
        "\n",
        "plt.bar(mean_survival_by_sex.index, mean_survival_by_sex.values)\n",
        "plt.title('Mean Survival Rate by Gender')\n",
        "plt.ylabel('Mean Survival Rate')\n",
        "plt.xlabel('Gender')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XhAyXa_7ptaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Problem - 5:\n",
        "\n",
        "embarked_survival = ds.groupby(\"Embarked\")[\"Survived\"].mean()\n",
        "print(embarked_survival)\n",
        "plt.bar(embarked_survival.index, embarked_survival.values)\n",
        "\n",
        "plt.xlabel(\"C, Q, S\")\n",
        "plt.ylabel(\"Survival Rate\")\n",
        "plt.title(\"Survival Rate by Port of Embarkation (C, Q, S)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5vaaEfyrpzLb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}